<Package>
<Extensions Backend="CPlusPlus" Condition="iOS">
       <Type Name="ViewFinder">
               <Set Source.FileExtension="mm" />
               <ProcessFile Build.SourceFile="-.CaptureWrapper.mm" />
               <ProcessFile Build.HeaderFile="-.CaptureWrapper.h" />
               <Require Source.Import="AVFoundation/AVFoundation.h" />
               <Require Source.Include="ImageIO/ImageIO.h" />
               <Require Source.Include="MobileCoreServices/MobileCoreServices.h" />
               <Require iOS.Build.Framework="ImageIO" />
               <Require iOS.Build.Framework="MobileCoreServices" />

               <Require Key="Source.Declaration">
                   <![CDATA[
                       #include "-.CaptureWrapper.h"
                   ]]>
               </Require>


           <Method Signature="SetSampleBuffer(VFIOS, iOS.AVFoundation.AVCaptureVideoDataOutput)">
              <Body>
                <![CDATA[
                  id delegate_id = (id)$0;
                  id output_id = $1->Handle();
                  dispatch_queue_t queue = dispatch_queue_create("myQueue", NULL);
                  [output_id setSampleBufferDelegate:delegate_id queue:queue];
                  dispatch_release(queue);
                ]]>
              </Body>
           </Method>
           <Method Signature="GetAVCaptureVideoDataOutput():ObjC.ID">
            <Body>
              <![CDATA[
                AVCaptureVideoDataOutput *output = [[[AVCaptureVideoDataOutput alloc] init] autorelease];
                output.videoSettings =
                            [NSDictionary dictionaryWithObject:
                                [NSNumber numberWithInt:kCVPixelFormatType_32BGRA]
                                forKey:(id)kCVPixelBufferPixelFormatTypeKey];
                // output.minFrameDuration = CMTimeMake(1, 15);
                return (id)output;
              ]]>
            </Body>
           </Method>
           <Method Signature="StartSession(iOS.AVFoundation.AVCaptureSession)">
            <Body>
              <![CDATA[
                id session = $0->Handle();
                [session startRunning];
              ]]>
            </Body>
           </Method>
           <Method Signature="showImage(ObjC.ID)">
            <Body>
              <![CDATA[
                if ($0 == nil) {
                 return;
                }
                uAutoReleasePool pool;
                ::app::iOS::UIKit::UIView* view = ::uPtr< ::app::iOS::UIKit::UIViewController*>(::uPtr< ::app::iOS::UIKit::UIWindow*>(::uPtr< ::app::iOS::UIKit::UIApplication*>(::app::iOS::UIKit::UIApplication___sharedApplication(NULL))->KeyWindow())->RootViewController())->View();
                ::UIView *nView = (::UIView *)view->Handle();
                UIImageView *myImageView = [[UIImageView alloc] init];
                myImageView.image = $0;
                [nView addSubview:myImageView];
              ]]>
            </Body>
           </Method>

           <Method Signature="textureFromSampleBuffer(ObjC.ID):Uno.Graphics.Texture2D">
            <Body>
              <![CDATA[
               if ($0 == nil) {
                return nil;
               }
               // Need a pool, since we are running on a different thread
               uAutoReleasePool pool;

               // Get a CMSampleBuffer's Core Video image buffer for the media data
               ::CMSampleBufferRef sampleBuffer = (::CMSampleBufferRef)$0;
               CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
               // Lock the base address of the pixel buffer
               CVPixelBufferLockBaseAddress(imageBuffer, 0);

               // Get the number of bytes per row for the pixel buffer
               void *baseAddress = CVPixelBufferGetBaseAddress(imageBuffer);

               // Get the number of bytes per row for the pixel buffer
               size_t bytesPerRow1 = CVPixelBufferGetBytesPerRow(imageBuffer);
               // Get the pixel buffer width and height
               size_t width1 = CVPixelBufferGetWidth(imageBuffer);
               size_t height1 = CVPixelBufferGetHeight(imageBuffer);

               // Create a device-dependent RGB color space
               ::CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();

               // Create a bitmap graphics context with the sample buffer data
               ::CGContextRef context1 = CGBitmapContextCreate(baseAddress, width1, height1, 8,
                 bytesPerRow1, colorSpace, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst);
               // Create a Quartz image from the pixel data in the bitmap graphics context
               CGImageRef quartzImage = CGBitmapContextCreateImage(context1);
               UIImage *image = [UIImage imageWithCGImage:quartzImage];
               NSData *imageData = UIImageJPEGRepresentation(image, 0.7f);
               NSString *encodedString = [imageData base64Encoding];
               NSString *javascript = @"data:image/jpeg;base64,";
               javascript = [javascript stringByAppendingString:encodedString];
               $$->PostString(javascript);


               NSUInteger width = CGImageGetWidth(quartzImage);
               NSUInteger height = CGImageGetHeight(quartzImage);
               NSUInteger bytesPerPixel = 4;
               NSUInteger bytesPerRow = bytesPerPixel * width;
               NSUInteger bitsPerComponent = 8;

               void *rawData = malloc(height * width * 4);

               ::CGContextRef context = CGBitmapContextCreate(
                   rawData, width, height, bitsPerComponent, bytesPerRow, colorSpace,
                   kCGImageAlphaPremultipliedLast | kCGBitmapByteOrder32Big);

               CGContextDrawImage(context, CGRectMake(0, 0, width, height), quartzImage);

               // Unlock the pixel buffer
               CVPixelBufferUnlockBaseAddress(imageBuffer,0);

               // Free up the context and color space
               CGContextRelease(context);
               CGContextRelease(context1);
               CGColorSpaceRelease(colorSpace);

               // Release the Quartz image
               CGImageRelease(quartzImage);

               // @{Uno.Int2} size = { width, height };
               @{Uno.Int2} size = { static_cast<int>(width), static_cast<int>(height) };
               @{Uno.Graphics.Texture2D} texture = @{Uno.Graphics.Texture2D(Uno.Int2,Uno.Graphics.Format,bool):Call(size,@{Uno.Graphics.Format.RGBA8888},false)};
               @{Uno.Graphics.Texture2D:Of(texture).Update(Uno.IntPtr):Call(rawData)};

               free(rawData);
               $$->PostTexture(texture);
               return texture;
              ]]>
            </Body>
           </Method>

           <Method Signature="SetupCaptureSessionImpl(VFIOS)">
               <Body>
                   <![CDATA[
                   NSError *error = nil;

                   // Create the session
                   AVCaptureSession *session = [[AVCaptureSession alloc] init];

                   // Configure the session to produce lower resolution video frames, if your
                   // processing algorithm can cope. We'll specify medium quality for the
                   // chosen device.
                   session.sessionPreset = AVCaptureSessionPresetMedium;

                   // Find a suitable AVCaptureDevice
                   AVCaptureDevice *device = [AVCaptureDevice
                                            defaultDeviceWithMediaType:AVMediaTypeVideo];

                   // Create a device input with the device and add it to the session.
                   AVCaptureDeviceInput *input = [AVCaptureDeviceInput deviceInputWithDevice:device
                                                                                   error:&error];

                   if (!input) {
                      return;
                      // Handling the error appropriately.
                   }
                   [session addInput:input];

                   // Create a VideoDataOutput and add it to the session
                   AVCaptureVideoDataOutput *output = [[[AVCaptureVideoDataOutput alloc] init] autorelease];
                   [session addOutput:output];

                   // Configure your output.
                   dispatch_queue_t queue = dispatch_queue_create("myQueue", NULL);
                   CaptureWrapper *cw = [[CaptureWrapper alloc] init];
                   [cw setViewFinderInst:$$];
                   [cw setSession:session];
                   [output setSampleBufferDelegate:cw queue:queue];
                   dispatch_release(queue);
                   // Specify the pixel format
                   output.videoSettings =
                               [NSDictionary dictionaryWithObject:
                                   [NSNumber numberWithInt:kCVPixelFormatType_32BGRA]
                                   forKey:(id)kCVPixelBufferPixelFormatTypeKey];

                   // If you wish to cap the frame rate to a known value, such as 15 fps, set
                   // minFrameDuration.
                   //output.minFrameDuration = CMTimeMake(1, 15);

                   /*
                   // This just shows the preview
                   ::app::iOS::UIKit::UIView* view = ::uPtr< ::app::iOS::UIKit::UIViewController*>(::uPtr< ::app::iOS::UIKit::UIWindow*>(::uPtr< ::app::iOS::UIKit::UIApplication*>(::app::iOS::UIKit::UIApplication___sharedApplication(NULL))->KeyWindow())->RootViewController())->View();

                   UIView *aView = (UIView *)view->Handle();
                   AVCaptureVideoPreviewLayer *previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:session];
                   previewLayer.frame = aView.bounds; // Assume you want the preview layer to fill the view.
                   [aView.layer addSublayer:previewLayer]; */

                   // Start the session running to start the flow of data
                   [session startRunning];

                   // Assign session to an ivar.
                   ::uPtr< ::app::VFIOS*>($0)->SessionID((id)session);
                   ]]>
               </Body>
           </Method>

       </Type>

</Extensions>
</Package>
